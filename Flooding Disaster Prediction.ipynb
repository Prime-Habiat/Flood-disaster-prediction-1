{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>Fundamentals of Artificial Intelligence and Knowledge Representation -\n",
    "Module 3</h3>\n",
    "    <h1>Flood disaster prediction</h1>\n",
    "    <h5>Antonio Politano, Francesco Pieroni and Riccardo Spolaor</h5>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries import and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3af5a064270f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes_grid1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_axes_locatable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# Suppress pgmpy internal deprecated use of third party libraries.\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Suppress UserWarning related to machine precision calculations of percentage.\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "# Import internal modules.\n",
    "from utils import *\n",
    "from variables import *\n",
    "from extended_classes import *\n",
    "from graphics import *\n",
    "\n",
    "# Import pgmpy modules.\n",
    "from pgmpy.factors.discrete.CPD import TabularCPD\n",
    "from pgmpy.inference import VariableElimination, ApproxInference\n",
    "from pgmpy.sampling import GibbsSampling\n",
    "\n",
    "# Import graphics related libraries and modules.\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Import other useful librares and modules.\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 50\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PER_UNIT_GDP = 'Per unit GDP'\n",
    "POPULATION_DENSITY = 'Population density'\n",
    "ROAD_DENSITY = 'Road density'\n",
    "ELEVATION = 'Elevation'\n",
    "SLOPE = 'Slope'\n",
    "RAINFALL_FREQUENCY = 'Rainfall frequency'\n",
    "RIVER_DENSITY = 'River density'\n",
    "RAINFALL_AMOUNT = 'Rainfall amount'\n",
    "FLOOD = 'Flood'\n",
    "\n",
    "variables = [\n",
    "    PER_UNIT_GDP,\n",
    "    POPULATION_DENSITY,\n",
    "    ROAD_DENSITY,\n",
    "    ELEVATION,\n",
    "    SLOPE,\n",
    "    RAINFALL_FREQUENCY,\n",
    "    RIVER_DENSITY,\n",
    "    RAINFALL_AMOUNT,\n",
    "    FLOOD\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_names_dictionary = {\n",
    "    PER_UNIT_GDP: ['High', 'Medium', 'Low'],\n",
    "    POPULATION_DENSITY: ['Dense', 'Medium', 'Sparse'],\n",
    "    ROAD_DENSITY: ['Dense', 'Medium', 'Sparse'],\n",
    "    ELEVATION: ['High', 'Medium', 'Low'],\n",
    "    SLOPE: ['Steep', 'Flat'],\n",
    "    RAINFALL_FREQUENCY: ['Frequent', 'Medium', 'Rare'],\n",
    "    RIVER_DENSITY: ['Dense', 'Sparse'],\n",
    "    RAINFALL_AMOUNT: ['Huge', 'Medium', 'Little'],\n",
    "    FLOOD: ['Yes', 'No']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining probability values to build all the CPDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_dictionary = {\n",
    "    PER_UNIT_GDP: [\n",
    "        [0.3], \n",
    "        [0.63], \n",
    "        [0.07]\n",
    "    ],\n",
    "    POPULATION_DENSITY: [\n",
    "        [0.7, 0.3, 0.05],\n",
    "        [0.2, 0.55, 0.25],\n",
    "        [0.1, 0.15, 0.7],\n",
    "    ],\n",
    "    ROAD_DENSITY: [\n",
    "        [0.8, 0.7, 0.1, 0.6, 0.4, 0.1, 0.1, 0.05, 0.01],\n",
    "        [0.19, 0.25, 0.35, 0.3, 0.5, 0.25, 0.25, 0.2, 0.15],\n",
    "        [0.01, 0.05, 0.55, 0.1, 0.1, 0.65, 0.65, 0.75, 0.84]\n",
    "    ],\n",
    "    ELEVATION: [\n",
    "        [0.15],\n",
    "        [0.1], \n",
    "        [0.75]\n",
    "    ],\n",
    "    SLOPE: [\n",
    "        [0.75, 0.6, 0.05], \n",
    "        [0.25, 0.4, 0.95]\n",
    "    ],\n",
    "    RAINFALL_FREQUENCY: [\n",
    "        [0.3], \n",
    "        [0.6], \n",
    "        [0.1]\n",
    "    ],\n",
    "    RIVER_DENSITY: [\n",
    "        [0.4], \n",
    "        [0.6]\n",
    "    ],\n",
    "    RAINFALL_AMOUNT: [\n",
    "        [0.7, 0.5, 0.55, 0.3, 0.1, 0.01],\n",
    "        [0.2, 0.25, 0.3, 0.4, 0.3, 0.04],\n",
    "        [0.1, 0.25, 0.15, 0.3, 0.6, 0.95]\n",
    "    ],\n",
    "    FLOOD: [\n",
    "        [0.07, 0.03, 0.005, 0.2, 0.1, 0.008, 0.05, 0.009, 0.0005, 0.13, 0.08, 0.006, 0.008, 0.002, 0.0001, 0.1, 0.04, 0.0002],\n",
    "        [0.93, 0.97, 0.995, 0.8, 0.9, 0.992, 0.95, 0.991, 0.9995, 0.87, 0.92, 0.994, 0.992, 0.998, 0.9999, 0.9, 0.96, 0.9998]\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [\n",
    "    (PER_UNIT_GDP, POPULATION_DENSITY),\n",
    "    (PER_UNIT_GDP, ROAD_DENSITY), \n",
    "    (POPULATION_DENSITY, ROAD_DENSITY),\n",
    "    (ROAD_DENSITY, FLOOD),\n",
    "    (ELEVATION, SLOPE),\n",
    "    (SLOPE, FLOOD),\n",
    "    (RAINFALL_FREQUENCY, RAINFALL_AMOUNT),\n",
    "    (RIVER_DENSITY, RAINFALL_AMOUNT),\n",
    "    (RAINFALL_AMOUNT, FLOOD)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_dictionary = {\n",
    "    PER_UNIT_GDP: None,\n",
    "    POPULATION_DENSITY: [PER_UNIT_GDP],\n",
    "    ROAD_DENSITY: [PER_UNIT_GDP, POPULATION_DENSITY],\n",
    "    ELEVATION: None,\n",
    "    SLOPE: [ELEVATION],\n",
    "    RAINFALL_FREQUENCY: None,\n",
    "    RIVER_DENSITY: None,\n",
    "    RAINFALL_AMOUNT: [RAINFALL_FREQUENCY, RIVER_DENSITY],\n",
    "    FLOOD: [ROAD_DENSITY, SLOPE, RAINFALL_AMOUNT]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpds = {v: get_tabular_cpd(v, state_names_dictionary, values_dictionary, evidence_dictionary) for v in variables}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k, v in cpds.items():\n",
    "    print('CPD Table for variable: {}'.format(k))\n",
    "    display(cpd_to_pandas(v))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model structure\n",
    "We can define the network by just passing a list of edges and then add the already defined CPDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtendedBayesianNetwork(edges)\n",
    "model.add_cpds(*[cpds[k] for k in cpds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The model has been correctly developed: {}.'.format(model.check_model()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model consists of the following Bayesian Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_bayesian_network(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indepencies\n",
    "Getting all the independencies given the parent nodes in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Independecies given the parent nodes shown using the function local_independencies():')\n",
    "model.local_independencies(variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Trails\n",
    "To see the flow of information, the active trail of each node is shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in variables:\n",
    "    print(model.active_trail_nodes(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V-Structure activation\n",
    "V - structure activated from the evidence \"Rainfall amount\", therefore river density influences rainfall frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_trail_nodes = model.active_trail_nodes(RIVER_DENSITY, observed=RAINFALL_AMOUNT)\n",
    "print(f'Trail of influence for variable {RIVER_DENSITY}: {active_trail_nodes}')\n",
    "display_v_structure(model, RIVER_DENSITY, RAINFALL_AMOUNT, active_trail_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of blocked flow of information, since Population density is shielded by \"Road density\" and \"Per unit GDP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.active_trail_nodes(POPULATION_DENSITY, observed=[ROAD_DENSITY, PER_UNIT_GDP]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is obtained since the given evidence is exactly the Markov Blanket of \"Population density\", which is confermed by the the pgmpy method get_markov_blanket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_markov_blanket(POPULATION_DENSITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing Markov Blankets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_markov_blanket (variable, markov_blanket):\n",
    "    color_map = []\n",
    "    for node in model.nodes:\n",
    "        if node == variable:\n",
    "            color_map.append('green')\n",
    "        elif node in markov_blanket: \n",
    "            color_map.append('red') \n",
    "        else:\n",
    "            color_map.append('blue')\n",
    "            \n",
    "    nx.draw(model, node_size = 2500, node_color=color_map, with_labels = True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for v in variables:\n",
    "    markov_blanket = model.get_markov_blanket(v)\n",
    "    display_markov_blanket (model, v, markov_blanket)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_infer = VariableElimination(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P(Flood)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_exact_inference(FLOOD, exact_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P(Flood | Road density = Dense)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_exact_inference(FLOOD, exact_infer, evidence={ROAD_DENSITY: 'Dense'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P(Flood | River density = Dense)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_exact_inference(FLOOD, exact_infer, evidence={RIVER_DENSITY: 'Dense'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P(Flood | Road density = Dense, Slope = Flat)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_exact_inference(FLOOD, exact_infer, evidence={ROAD_DENSITY: 'Dense', SLOPE: 'Flat'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P(Flood | Road density = Dense, Slope = Flat, Rainfall Frequency = Frequent)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_exact_inference(FLOOD, exact_infer, evidence={ROAD_DENSITY: 'Dense', SLOPE: 'Flat', RAINFALL_FREQUENCY: 'Frequent'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P(Flood | Road density = Medium, Slope = Flat, Rainfall Frequency = Frequent)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_exact_inference(FLOOD, exact_infer, evidence={ROAD_DENSITY: 'Medium', SLOPE: 'Flat', RAINFALL_FREQUENCY: 'Frequent'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P(Flood | Road density = Dense, Slope = Steep, Rainfall Frequency = Frequent)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_exact_inference(FLOOD, exact_infer, evidence={ROAD_DENSITY: 'Dense', SLOPE: 'Steep', RAINFALL_FREQUENCY: 'Frequent'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P(Slope)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_exact_inference(SLOPE, exact_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P(Slope | Flood = Yes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_exact_inference(SLOPE, exact_infer, evidence={FLOOD: 'Yes'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P(Rainfall Amount | Flood = Yes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_exact_inference(RAINFALL_AMOUNT, exact_infer, evidence={FLOOD: 'Yes'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P(Rainfall Frequency | Rainfall Amount = Huge)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_exact_inference(RAINFALL_FREQUENCY, exact_infer, evidence={RAINFALL_AMOUNT: 'Huge'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P(Rainfall Frequency | Rainfall Amount = Little)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_exact_inference(RAINFALL_FREQUENCY, exact_infer, evidence={RAINFALL_AMOUNT: 'Little'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Inference\n",
    "\n",
    "This section evaluates the approximate inference on a series of variables given evidence. The calculation is performed by using the Approximate Inference method `ApproxInference.query` which applies the *Negation Sampling*. The results are compared with their *Exact Inference* counterpart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Negation Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_infer_sampling = ExtendedApproxInference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P(Flood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_exact_inference(FLOOD, exact_infer)\n",
    "print()\n",
    "print_approximate_inference(FLOOD, approx_infer_sampling, n_samples=1_000, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows that approximate inference gives a very good estimation of the probability distribution of Flood, since the values are very close the ones we get with exact inference. The main reason is that we used a very high number of samples, which approximates well the limit to infinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P(Flood | Road Density = Dense, Slope = Steep, Rainfall Frequency = Frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_exact_inference(FLOOD, exact_infer, evidence={ROAD_DENSITY: 'Dense', SLOPE: 'Steep', RAINFALL_FREQUENCY: 'Frequent'})\n",
    "\n",
    "print_approximate_inference(FLOOD, \n",
    "                            approx_infer_sampling, \n",
    "                            evidence={ROAD_DENSITY: 'Dense', SLOPE: 'Steep', RAINFALL_FREQUENCY: 'Frequent'}, \n",
    "                            n_samples=1_000, \n",
    "                            random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the case above, the estimation using approximate inference with negation sampling is very good, since we used many samples. Unfortunatelly, this results in having a longer execution time than using exact inference, so there is no benefit in using approximate inference with those many samples.\n",
    "Rather, we can dramatically decrease the number of samples and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_approximate_inference(FLOOD, approx_infer_sampling, \n",
    "                            evidence={ROAD_DENSITY: 'Dense', SLOPE: 'Steep', RAINFALL_FREQUENCY: 'Frequent'}, \n",
    "                            n_samples=50, \n",
    "                            random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate inference with negation sampling does not work quiet well, giving results worse than the second case where more samples were given. In addition, the execution time has not changed a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second case\n",
    "#### P(Flood | Per unit GDP = Low, Elevation = Medium, Rainfall Frequency = Rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"start_time = time.time_ns()\n",
    "\n",
    "print(\"Exact Inference to find P(Flood | Per unit GDP = Dense, Slope = Flat, Rainfall Frequency = Frequent)\\n\")\n",
    "print(exact_infer.query([FLOOD], \n",
    "                        evidence={PER_UNIT_GDP: 'Low', ELEVATION: 'Medium', RAINFALL_FREQUENCY: 'Rare'}, \n",
    "                        show_progress=False\n",
    "))\n",
    "\n",
    "print(f\"--- {(time.time_ns() - start_time) / 1_000_000_000} seconds ---\")\n",
    "\n",
    "start_time = time.time_ns()\n",
    "\n",
    "print(\"Approximate Inference with sampling to find P(Flood)\\n\")\n",
    "print(approx_infer_sampling.query(variables=[FLOOD], \n",
    "                                  evidence={PER_UNIT_GDP: 'Low', ELEVATION: 'Medium', RAINFALL_FREQUENCY: 'Rare'}, \n",
    "                                  n_samples=1_000, show_progress=False, seed=RANDOM_STATE\n",
    "))\n",
    "\n",
    "\n",
    "print(f\"--- {(time.time_ns() - start_time)  / 1_000_000_000} seconds ---\")\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the case above, the estimation using approximate inference with negation sampling is very good, since we used many samples. Unfortunatelly, this results in having a longer execution time than using exact inference, so there is no benefit in using approximate inference with those many samples.\n",
    "Rather, we can dramatically decrease the number of samples and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third case (few samples)\n",
    "#### P(Flood | Per unit GDP = Low, Elevation = Medium, Rainfall Frequency = Rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"start_time = time.time_ns()\n",
    "\n",
    "print(\"Exact Inference to find P(Flood | Per unit GDP = Dense, Slope = Flat, Rainfall Frequency = Frequent)\\n\")\n",
    "print(exact_infer.query([FLOOD], \n",
    "                        evidence={PER_UNIT_GDP: 'Low', ELEVATION: 'Medium', RAINFALL_FREQUENCY: 'Rare'}, \n",
    "                        show_progress=False\n",
    "))\n",
    "\n",
    "print(f\"--- {(time.time_ns() - start_time) / 1_000_000_000} seconds ---\")\n",
    "\n",
    "start_time = time.time_ns()\n",
    "\n",
    "print(\"Approximate Inference with sampling to find P(Flood)\\n\")\n",
    "print(approx_infer_sampling.query(variables=[FLOOD], \n",
    "                                  evidence={PER_UNIT_GDP: 'Low', ELEVATION: 'Medium', RAINFALL_FREQUENCY: 'Rare'}, \n",
    "                                  n_samples=50, show_progress=False, seed=RANDOM_STATE\n",
    "))\n",
    "\n",
    "\n",
    "print(f\"--- {(time.time_ns() - start_time)  / 1_000_000_000} seconds ---\")\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate inference with negation sampling does not work quiet well, giving results worse than the second case where more samples were given. In addition, the execution time has not changed a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P(Flood | Road density = Dense, Slope = Flat, Rainfall Frequency = Frequent)\n",
    "* Let's discuss the fact that the approximate inference should be maybe quicker rather than exact inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"start_time = time.time_ns()\n",
    "\n",
    "print(\"Exact Inference to find P(Flood|Road density = Dense, Slope = Flat, Rainfall Frequency = Frequent)\\n\")\n",
    "print(exact_infer.query([FLOOD], \n",
    "                        evidence={ROAD_DENSITY: 'Dense', SLOPE: 'Steep', RAINFALL_FREQUENCY: 'Frequent'}, \n",
    "                        show_progress=False\n",
    "))\n",
    "\n",
    "print(f\"--- {(time.time_ns() - start_time) / 1_000_000_000} seconds ---\")\n",
    "\n",
    "start_time = time.time_ns()\n",
    "\n",
    "print(\"Approximate Inference with sampling to find P(Flood)\\n\")\n",
    "print(approx_infer_sampling.query(variables=[FLOOD], \n",
    "                                  evidence={ROAD_DENSITY: 'Dense', SLOPE: 'Steep', RAINFALL_FREQUENCY: 'Frequent'}, \n",
    "                                  n_samples=50, show_progress=False, seed=RANDOM_STATE\n",
    "))\n",
    "\n",
    "\n",
    "print(f\"--- {(time.time_ns() - start_time)  / 1_000_000_000} seconds ---\")\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the case above, the estimation using approximate inference is very good, since we used many samples. Unfortunatelly, this results in having a longer execution time than using exact inference, so there is no benefit in using approximate inference with those many samples.\n",
    "Rather, we can dramatically decrease the number of samples and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"start_time = time.time_ns()\n",
    "\n",
    "print(\"Approximate Inference with sampling to find P(Flood)\\n\")\n",
    "print(approx_infer_sampling.query(variables=[FLOOD], evidence={ROAD_DENSITY: 'Dense', SLOPE: 'Steep', RAINFALL_FREQUENCY: 'Frequent'}, n_samples=1000, show_progress=False, seed=RANDOM_STATE))\n",
    "\n",
    "print(f\"--- {(time.time_ns() - start_time)  / 1_000_000_000} seconds ---\")\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate inference still works quiet well, but gives results worse than the first try with more samples. In addition, the execution time is perfectly comparable to the one of exact inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Likelihood Weighting\n",
    "\n",
    "This section evaluates the approximate inference on a series of variables given evidence using the Likelihood Weighting method. The calculation is performed by using the `ApproxInferenceWeightedSampling.query` which extends the `ApproxInference.query` method by giving the option to use the *Likelihood Weighting* method on the *Bayesian Model*. The results are compared with their *Exact Inference* counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_infer_sampling = ExtendedApproxInference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P(Flood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_exact_inference(FLOOD, exact_infer)\n",
    "print()\n",
    "print_approximate_inference(FLOOD, \n",
    "                            approx_infer_sampling, \n",
    "                            n_samples=1_000, \n",
    "                            random_state=RANDOM_STATE, \n",
    "                            use_weighted_likelihood=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows that approximate inference gives a very good estimation of the probability distribution of Flood, since the values are very close the ones we get with exact inference. The main reason is that we used a very high number of samples, which approximates well the limit to infinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P(Flood | Road Density = Dense, Slope = Steep, Rainfall Frequency = Frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_exact_inference(FLOOD, \n",
    "                      exact_infer, \n",
    "                      evidence={ROAD_DENSITY: 'Dense', SLOPE: 'Steep', RAINFALL_FREQUENCY: 'Frequent'})\n",
    "\n",
    "print()\n",
    "\n",
    "print_approximate_inference(FLOOD, \n",
    "                            approx_infer_sampling,\n",
    "                            evidence={ROAD_DENSITY: 'Dense', SLOPE: 'Steep', RAINFALL_FREQUENCY: 'Frequent'}, \n",
    "                            n_samples=1_000, \n",
    "                            random_state=RANDOM_STATE, \n",
    "                            use_weighted_likelihood=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the case above, the estimation using approximate inference with negation sampling is very good, since we used many samples. Unfortunatelly, this results in having a longer execution time than using exact inference, so there is no benefit in using approximate inference with those many samples.\n",
    "Rather, we can dramatically decrease the number of samples and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_approximate_inference(FLOOD, \n",
    "                            approx_infer_sampling, \n",
    "                            evidence={ROAD_DENSITY: 'Dense', SLOPE: 'Steep', RAINFALL_FREQUENCY: 'Frequent'}, \n",
    "                            n_samples=50, \n",
    "                            random_state=RANDOM_STATE, \n",
    "                            use_weighted_likelihood=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate inference with weighted likelihood sampling does not work quiet well, giving results worse than the second case where more samples were given. In addition, the execution time has not changed a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P(Flood | Per unit GDP = Low, Elevation = Medium, Rainfall Frequency = Rare)\n",
    "* Let's discuss the fact that the approximate inference should be maybe quicker rather than exact inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"start_time = time.time_ns()\n",
    "\n",
    "evidence_to_use = {PER_UNIT_GDP: 'Low', ELEVATION: 'Medium', RAINFALL_FREQUENCY: 'Rare'}\n",
    "\n",
    "print(\"Exact Inference to find P(Flood|Road density = Dense, Slope = Flat, Rainfall Frequency = Frequent)\\n\")\n",
    "print(exact_infer.query([FLOOD], evidence=evidence_to_use, show_progress=False))\n",
    "\n",
    "print(f\"--- {(time.time_ns() - start_time) / 1_000_000_000} seconds ---\")\n",
    "\n",
    "start_time = time.time_ns()\n",
    "\n",
    "print(\"Approximate Inference with sampling to find P(Flood)\\n\")\n",
    "print(approx_infer_sampling.query(variables=[FLOOD], evidence=evidence_to_use, n_samples=100, show_progress=False, \n",
    "                                  use_weighted_sampling=True, seed=RANDOM_STATE))\n",
    "\n",
    "\n",
    "print(f\"--- {(time.time_ns() - start_time)  / 1_000_000_000} seconds ---\")\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Markov Chain Monte Carlo Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gibbs = GibbsSampling(model)\n",
    "gibbs_df = gibbs.sample(size=10_000, seed=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gibbs_df['Flood'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P(Flood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_Flood = gibbs_df['Flood'].value_counts()\n",
    "\n",
    "#Flood  Yes:0, No:1\n",
    "print('Phi(Flood = Yes): ',series_Flood[0]/(sum(series_Flood)) )\n",
    "print('Phi(Flood = No): ',series_Flood[1]/(sum(series_Flood)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P(Flood  | Per unit GDP = High)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GDP High: 0, Medium: 1, Low: 2\n",
    "series_Flood_GDP = gibbs_df.loc[gibbs_df['Per unit GDP'] == 0]['Flood'].value_counts()\n",
    "\n",
    "#Flood  Yes:0, No:1\n",
    "print('Phi(Flood = Yes | Per unit GDP = High): ',series_Flood_GDP[0]/(sum(series_Flood_GDP)) )\n",
    "print('Phi(Flood = No | Per unit GDP = High): ',series_Flood_GDP[1]/(sum(series_Flood_GDP)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case\n",
    "\n",
    "In this section an use case for the implemented Bayesian Network is presented. \n",
    "\n",
    "We decided to test the performance of our qualitative built network by using relevant information related to each municipality of the Italian Veneto region.\n",
    "In particular, the chance of flooding on a yearly basis of each municipality is computed through *Exact inference* given as evidence frequent rainfalls for the period (**Rainfall Frequency = Frequent**) and each municipality information about the **Per unit GDP**, **Population density**, **Slope** and **River density**.\n",
    "\n",
    "$$ \\forall \\ m \\in \\text{Municipalities}, P(\\text{Flood}_m\\ | \\ \\text{Per unit GDP}_m, \\text{Population density}_m, \\text{Slope}_m,\\text{River density}_m, \\text{Rainfall frequency = Frequent})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Municipalities data\n",
    "The municipalities data needed to perform the inference is obtained throgh different reliabe sources (e.g.: the *Ministry of Economy and Finance* of Italy, the *Italian National Institute of Statistics* or the Veneto Region official web platform) and it is collected in a `pandas` `DataFrame`.\n",
    "\n",
    "The sources of each piece of information are expressed below:\n",
    "* Municipalities general information and regional incomes: https://www1.finanze.gov.it/finanze/analisi_stat/public/index.php?tree=2020\n",
    "* Municipalities surfaces: https://data.europa.eu/data/datasets/superficie_territoriale_in_kmq_dei_comuni_del_veneto?locale=it\n",
    "* Municipalities population: https://www.istat.it/it/archivio/243448\n",
    "* Municipalities water surfaces: https://www.regione.veneto.it/web/ambiente-e-territorio/scheda-dati\n",
    "* Municipalities altitudes: https://www.istat.it/it/archivio/156224\n",
    "\n",
    "Missing data has been filled according to the municipalities information given by the following websites:\n",
    "* https://it.wikipedia.org/wiki/Pagina_principale\n",
    "* https://www.cittaeborghi.it/it/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful constants for data manipulation are expressed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISTAT_VENETO_CODE = 5\n",
    "MUNICIPALITY_NAME_COLUMN = 'Municipality Name'\n",
    "ISTAT_CODE_COLUMN = 'Istat code'\n",
    "PROVINCE_COLUMN = 'Province'\n",
    "SURFACE_COLUMN = 'Surface (km2)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main dataframe\n",
    "The main dataframe (`df`) is created by exploiting the information given by the region's income data (`income_df`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `income_df` is created and inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df = pd.read_csv('./data/Redditi_e_principali_variabili_IRPEF_su_base_comunale_CSV_2019.csv', sep=';', index_col=False)\n",
    "income_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`income_df` is reduced to the sole municipalities of the Veneto region, which are 563."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df = income_df[income_df['Codice Istat Regione'] == ISTAT_VENETO_CODE]\n",
    "income_df.reset_index(inplace=True)\n",
    "income_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main dataframe (`df`) is created using the `income_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = income_df[['Denominazione Comune', 'Codice Istat Comune', 'Sigla Provincia']]\n",
    "\n",
    "df = df.rename(columns={\n",
    "    'Denominazione Comune': MUNICIPALITY_NAME_COLUMN, \n",
    "    'Codice Istat Comune': ISTAT_CODE_COLUMN, \n",
    "    'Sigla Provincia': PROVINCE_COLUMN\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values are present in the newly created `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of NaN values in df: \\n{}'.format(df.isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per unit GDP information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information needed to computed the **Per unit GDP** values is inspected, and it is available for each municipality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of NaN values regarding the total amount of income: {}'.format(\n",
    "    income_df['Reddito imponibile - Ammontare in euro'].isna().sum())\n",
    "     )\n",
    "print('Number of NaN values regarding the total amount of tax payers: {}'.format(income_df['Numero contribuenti'].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Per unit GDP** data is computed by dividing for each region the total income by the number of tax payers given by the `income_df` and the column is added to the main `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[PER_UNIT_GDP] = income_df['Reddito imponibile - Ammontare in euro'] / income_df['Numero contribuenti']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population density information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information needed to computed the **Population density** values is obtained by firstly creating the `population_density_df` and then merging the density per $km^2$ information into the main `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_density_df = pd.read_excel('./data/05_Veneto_Allegato-statistico.xlsx', 'Appendice 2',  skiprows = 10)\n",
    "population_density_df = population_density_df[['ProCom', 'Densità']]\n",
    "population_density_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, population_density_df, left_on=ISTAT_CODE_COLUMN, right_on='ProCom', how='left').drop('ProCom', axis=1)\n",
    "\n",
    "df = df.rename(columns={'Densità': POPULATION_DENSITY})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing information is filled and the main `df` is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows of df where \"{}\" is NaN.'.format(POPULATION_DENSITY))\n",
    "\n",
    "df[df[POPULATION_DENSITY].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[558, POPULATION_DENSITY] = 80.15\n",
    "df.loc[559, POPULATION_DENSITY] = 175.74\n",
    "df.loc[560, POPULATION_DENSITY] = 53.28\n",
    "df.loc[561, POPULATION_DENSITY] = 307.63\n",
    "df.loc[562, POPULATION_DENSITY] = 76.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slope information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information needed to computed the **Slope** values is obtained by firstly creating the `elevation_df`, next and then merging the altitude range information of each municipality (expressed in $mts$) into the main `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_df = pd.read_excel('./data/Elab_Altimetrie_DEM.xlsx')\n",
    "elevation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_df = elevation_df[elevation_df['COD_REG'] == ISTAT_VENETO_CODE]\n",
    "elevation_df = elevation_df.rename(columns={'RANGE': SLOPE,})\n",
    "elevation_df = elevation_df[['PRO_COM', SLOPE]]\n",
    "\n",
    "df = pd.merge(\n",
    "    df, \n",
    "    elevation_df, \n",
    "    left_on=ISTAT_CODE_COLUMN,\n",
    "    right_on='PRO_COM', \n",
    "    how = 'left'\n",
    ").drop('PRO_COM', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing information is filled and the main `df` is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows of df where \"{}\" is NaN.'.format(SLOPE))\n",
    "\n",
    "df[df[SLOPE].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[551, SLOPE] = 1625 - 175\n",
    "df.loc[552, SLOPE] = 2502 - 400 \n",
    "df.loc[553, SLOPE] = 3210 - 593\n",
    "df.loc[554, SLOPE] = 2471 - 380\n",
    "df.loc[555, SLOPE] = 320 - 15\n",
    "df.loc[556, SLOPE] = 445 - 15\n",
    "df.loc[557, SLOPE] = 22 - 7\n",
    "\n",
    "# Ulterior missing (and unavailable) information was filled by the mean value of \"Slope\" across the whole dataset.\n",
    "df[SLOPE].fillna(df[SLOPE].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### River density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information needed to computed the **River density** values is obtained by firstly creating the `river_density_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_density_df = pd.read_excel('./data/codiceISTAT_schedaLR14_2017.ods', 'ISTAT')\n",
    "river_density_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the **River density** information is obtained by dividing the surface of water of each municipality (`classe_5`) by its surface (`Superficie Territoriale`) and the `river_density_df` dataset is reduced with the solely needed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_density_df[RIVER_DENSITY] = river_density_df['classe_5'] / river_density_df['Superficie Territoriale']\n",
    "river_density_df = river_density_df[['cod. comune', RIVER_DENSITY]]\n",
    "river_density_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **River density** information of each municipality (expressed in a value between 0 and 1) is merged into the main `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    df, \n",
    "    river_density_df, \n",
    "    left_on=ISTAT_CODE_COLUMN,\n",
    "    right_on='cod. comune', \n",
    "    how = 'left'\n",
    ").drop('cod. comune', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing information is filled by the mean value of **River density** across the dataset and the main `df` is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows of df where \"{}\" is NaN.'.format(RIVER_DENSITY))\n",
    "\n",
    "df[df[RIVER_DENSITY].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[RIVER_DENSITY].fillna(df[RIVER_DENSITY].mean(), inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data discretization\n",
    "The municipalities data collected in `df` is discretized according to the information used to build the *Bayesian Network* or their *exact inference* when the variables have parent nodes in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_unit_gdp_inference = exact_infer.query([PER_UNIT_GDP], show_progress=False)\n",
    "population_density_inference = exact_infer.query([POPULATION_DENSITY], show_progress=False)\n",
    "slope_inference = exact_infer.query([SLOPE], show_progress=False)\n",
    "river_density_inference = exact_infer.query([RIVER_DENSITY], show_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The infered data shown above is used to discretize the `df` data according to specific quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_discrete_values(PER_UNIT_GDP, df, [\n",
    "    df[PER_UNIT_GDP].min(),\n",
    "    df[PER_UNIT_GDP].quantile(per_unit_gdp_inference.values[-1]),\n",
    "    df[PER_UNIT_GDP].quantile(1 - per_unit_gdp_inference.values[0]),\n",
    "    df[PER_UNIT_GDP].max()\n",
    "], state_names_dictionary)\n",
    "\n",
    "apply_discrete_values(SLOPE, df, [\n",
    "    df[SLOPE].min(),\n",
    "    df[SLOPE].quantile(slope_inference.values[-1]),\n",
    "    df[SLOPE].max()\n",
    "], state_names_dictionary)\n",
    "\n",
    "apply_discrete_values(POPULATION_DENSITY, df, [\n",
    "    df[POPULATION_DENSITY].min(),\n",
    "    df[POPULATION_DENSITY].quantile(population_density_inference.values[-1]),\n",
    "    df[POPULATION_DENSITY].quantile(1 - population_density_inference.values[0]),\n",
    "    df[POPULATION_DENSITY].max()\n",
    "], state_names_dictionary)\n",
    "\n",
    "apply_discrete_values(RIVER_DENSITY, df, [\n",
    "    df[RIVER_DENSITY].min(),\n",
    "    df[RIVER_DENSITY].quantile(river_density_inference.values[-1]),\n",
    "    df[RIVER_DENSITY].max()\n",
    "], state_names_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood of flooding\n",
    "\n",
    "The chance of flooding on a yearly basis of each municipality is computed through *Exact inference* given as evidence frequent rainfalls for the period (**Rainfall Frequency = Frequent**) and each municipality information about the **Per unit GDP**, **Population density**, **Slope** and **River density**.\n",
    "\n",
    "$$ \\forall \\ m \\in \\text{Municipalities}, P(\\text{Flood}_m\\ | \\ \\text{Per unit GDP}_m, \\text{Population density}_m, \\text{Slope}_m,\\text{River density}_m, \\text{Rainfall frequency = Frequent})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['flood_likelihood'] = df.apply(\n",
    "    lambda row: exact_infer.query([FLOOD], evidence={\n",
    "        PER_UNIT_GDP: row[PER_UNIT_GDP], \n",
    "        POPULATION_DENSITY: row[POPULATION_DENSITY], \n",
    "        SLOPE: row[SLOPE], \n",
    "        RIVER_DENSITY: row[RIVER_DENSITY],\n",
    "        RAINFALL_FREQUENCY: 'Frequent'\n",
    "    }, show_progress=False).get_value(Flood='Yes'), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `geopandas_df` is created in order to represent each municipality as a geometrical 2-dimensional shape and to plot a map of the Veneto region divided in municipalities with information about the flood chances.\n",
    "\n",
    "The data has been fetched from the following site:\n",
    "* https://github.com/openpolis/geojson-italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geopandas_df = gpd.read_file('https://raw.githubusercontent.com/openpolis/geojson-italy/master/geojson/limits_R_5_municipalities.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geopandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of municipalities of `geopandas_df` corresponds to the ones of `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((geopandas_df.shape[0], df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Flood likelihood** information for each municipality (expressed in a value between 0 and 1) is merged into the `geopandas_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geopandas_df = pd.merge(\n",
    "    geopandas_df, \n",
    "    df[['Istat code','flood_likelihood']], \n",
    "    left_on='com_istat_code_num',\n",
    "    right_on='Istat code', \n",
    "    how = 'left'\n",
    ").drop('Istat code', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geopandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flood chances heatmap\n",
    "\n",
    "Finally, a heatmap showing the probability of flood for each municipality is shown.\n",
    "\n",
    "The results are significative, since it is evdent how the northern areas where the slope of the surface is steeper have lower chances of being flooded, although there may be some exception, like in places near *Belluno*, which are zones with a high elevation, but with a quite flat surface resulting in higher probabilities of flooding.\n",
    "\n",
    "More densely populated and richer areas such as some chief towns (*Venezia*, *Verona*, *Vicenza*, *Padova* and *Rovigo*) have the highest likelihood of experiencing floods, whereas poorer and less populated although quietly flat areas in the southern regions show medium probabilities of being flooded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"2%\", pad=-1)\n",
    "\n",
    "ax = geopandas_df.plot(column='flood_likelihood', cmap='seismic', ax=ax, legend= True, cax=cax)\n",
    "\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "\n",
    "provinces = ['Venezia', 'Verona', 'Vicenza', 'Padova', 'Treviso', 'Rovigo', 'Belluno']\n",
    "\n",
    "for idx, row in geopandas_df[geopandas_df['name'].isin(provinces)].iterrows():\n",
    "    coords = row['geometry'].representative_point().coords[:]\n",
    "    coords = coords[0]\n",
    "    ax.annotate(row['prov_acr'], xy=coords, horizontalalignment='center')\n",
    "    \n",
    "fig.suptitle('Flood probability in Veneto region by municipality', y=0.86, x=0.56)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
